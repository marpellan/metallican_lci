{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd"
   ],
   "id": "e84bfa8680c3250c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Custom functions\n",
    "from some_functions import merge_without_suffixes, normalize_by_production, get_info_for_ids, create_sankey_diagram"
   ],
   "id": "694c4ed4e38e05b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metallican_path = r'C:\\Users\\mp_ma\\OneDrive - polymtlus\\Desktop\\POST_DOC\\Project\\canada_metal_sustainability_db'",
   "id": "1cfdf8882e94433a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import MetalliCan tables",
   "id": "2337d4e536d4b48f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "main_table = pd.read_csv(metallican_path + r'\\database\\CSV\\main_table.csv')\n",
    "production_table = pd.read_csv(metallican_path + r'\\database\\CSV\\production_table.csv')\n",
    "tech_attributes_table = pd.read_csv(metallican_path + r'\\database\\CSV\\tech_attributes_table.csv')\n",
    "env_table = pd.read_csv(metallican_path + r'\\database\\CSV\\environment_table_new.csv')\n",
    "technosphere_table = pd.read_csv(metallican_path + r'\\database\\CSV\\technosphere_table.csv')\n",
    "archetypes_table = pd.read_csv(metallican_path + r'\\database\\CSV\\archetypes_table.csv')\n",
    "land_table = pd.read_csv(metallican_path + r'\\database\\CSV\\land_occupation_table.csv')"
   ],
   "id": "1c1025a88330e847",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create samples based on the best available data",
   "id": "58d1dd07f2ce7441"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ids_production = set(production_table[['main_id', 'facility_group_id']].apply(tuple, axis=1))\n",
    "ids_ta = set(tech_attributes_table[['main_id', 'facility_group_id']].apply(tuple, axis=1))\n",
    "ids_environmental = set(env_table[['main_id', 'facility_group_id']].apply(tuple, axis=1))\n",
    "ids_technosphere = set(technosphere_table[['main_id', 'facility_group_id']].apply(tuple, axis=1))\n",
    "ids_archetypes = set(archetypes_table[['main_id', 'facility_group_id']].apply(tuple, axis=1))"
   ],
   "id": "40d8c16d6012a551",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find the intersection of these sets to get common IDs\n",
    "best_ids = ids_production & ids_ta & ids_environmental & ids_technosphere\n",
    "second_best_ids = ids_production & ids_ta & ids_environmental\n",
    "third_best_ids = ids_production & ids_technosphere"
   ],
   "id": "1ba35751d82b99b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "main_best_df = get_info_for_ids(main_table, best_ids)\n",
    "main_second_best_df = get_info_for_ids(main_table, second_best_ids)\n",
    "main_third_best_df = get_info_for_ids(main_table, third_best_ids)"
   ],
   "id": "61daac7dd6528381",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Export the best and second best IDs to Excel\n",
    "with pd.ExcelWriter(r'data\\MetalliCan\\sample_ids.xlsx') as writer:\n",
    "    main_best_df.to_excel(writer, sheet_name='ta_technosphere_biosphere', index=False)\n",
    "    main_second_best_df.to_excel(writer, sheet_name='ta_biosphere', index=False)\n",
    "    main_third_best_df.to_excel(writer, sheet_name='technosphere', index=False)"
   ],
   "id": "6552a93968887d19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "m# Get the mining method and submethod for the different samples",
   "id": "fc495e56f06e04cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_best_df",
   "id": "a4f0f962ac5bb4c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a5035dcb4c9cda96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "archetypes_best = get_info_for_ids(archetypes_table, best_ids)[['main_id', 'facility_group_id', 'ore_type', 'mining_method', 'mining_submethod']]\n",
    "archetypes_second_best = get_info_for_ids(archetypes_table, second_best_ids)[['main_id', 'facility_group_id', 'ore_type', 'mining_method', 'mining_submethod']]\n",
    "archetypes_third_best = get_info_for_ids(archetypes_table, third_best_ids)[['main_id', 'facility_group_id', 'ore_type', 'mining_method', 'mining_submethod']]"
   ],
   "id": "5e334b8a542871b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best = merge_without_suffixes(main_best_df, archetypes_best, keys=(\"main_id\", \"facility_group_id\"), how=\"left\")\n",
    "second_best = merge_without_suffixes(main_second_best_df, archetypes_second_best, keys=(\"main_id\", \"facility_group_id\"), how=\"left\")\n",
    "third_best = merge_without_suffixes(main_third_best_df, archetypes_third_best, keys=(\"main_id\", \"facility_group_id\"), how=\"left\")"
   ],
   "id": "db1fcaf5b1309b42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sankey_best = create_sankey_diagram(best)\n",
    "sankey_best.write_html(r'data\\MetalliCan\\sample_best.html', )"
   ],
   "id": "cf6fbba84615c9ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sankey_third = create_sankey_diagram(third_best)\n",
    "sankey_third.write_html(r'data\\MetalliCan\\sample_third.html', )"
   ],
   "id": "91f940fa9f831013",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create parametrization table for the best IDs",
   "id": "2725be687686518e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "production_df_best = get_info_for_ids(production_table, best_ids)\n",
    "production_df_second = get_info_for_ids(production_table,second_best_ids)\n",
    "production_df_third = get_info_for_ids(production_table,third_best_ids)"
   ],
   "id": "6aa5e9e52a9fd162",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove rows where prod_id do not start with 'PROD' and only keep 'Crude ore' reference point\n",
    "production_df_best = production_df_best[\n",
    "    (production_df_best['prod_id'].str.startswith('PROD')) &\n",
    "    (production_df_best['reference_point'] == 'Crude ore')\n",
    "]"
   ],
   "id": "d724de636b7819a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "production_df_second = production_df_second[\n",
    "    (production_df_second['prod_id'].str.startswith('PROD')) &\n",
    "    (production_df_second['reference_point'] == 'Crude ore')\n",
    "]"
   ],
   "id": "4556808229883462",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "production_df_third = production_df_third[\n",
    "    (production_df_third['prod_id'].str.startswith('PROD')) &\n",
    "    (production_df_third['reference_point'] == 'Crude ore')\n",
    "]"
   ],
   "id": "33ed3565f23061d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ta_best = get_info_for_ids(tech_attributes_table, best_ids)[['main_id', 'facility_group_id', 'commodity', 'material_type', 'unit', 'value']]\n",
    "ta_second_best = get_info_for_ids(tech_attributes_table, second_best_ids)[['main_id', 'facility_group_id', 'commodity', 'material_type', 'unit', 'value']]"
   ],
   "id": "5c559ba06512de02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "archetypes_best = get_info_for_ids(archetypes_table, best_ids)[['main_id', 'facility_group_id', 'ore_type', 'mining_method', 'mining_submethod']]\n",
    "archetypes_second_best = get_info_for_ids(archetypes_table, second_best_ids)[['main_id', 'facility_group_id', 'ore_type', 'mining_method', 'mining_submethod']]\n",
    "archetypes_third_best = get_info_for_ids(archetypes_table, third_best_ids)[['main_id', 'facility_group_id', 'ore_type', 'mining_method', 'mining_submethod']]"
   ],
   "id": "9cd1f34749d51157",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def convert_to_percent(row):\n",
    "    if row['unit'] == 'g/t':\n",
    "        return row['value'] / 10000\n",
    "    elif row['unit'] == '%':\n",
    "        return row['value']\n",
    "    else:\n",
    "        return None\n"
   ],
   "id": "3d2bd3647e03d1d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ta_best['value_%'] = ta_best.apply(convert_to_percent, axis=1)\n",
    "ta_best.drop(columns=['unit', 'value'], inplace=True)"
   ],
   "id": "1b61baedae2ab619",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ta_second_best['value_%'] = ta_second_best.apply(convert_to_percent, axis=1)\n",
    "ta_second_best.drop(columns=['unit', 'value'], inplace=True)"
   ],
   "id": "54a07f785d751e82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parametrization_best = merge_without_suffixes(ta_best, archetypes_best, keys=(\"main_id\", \"facility_group_id\"), how=\"left\")\n",
    "parametrization_second_best = merge_without_suffixes(ta_second_best, archetypes_second_best, keys=(\"main_id\", \"facility_group_id\"), how=\"left\")"
   ],
   "id": "575bb0725094fe89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parametrization_best",
   "id": "5d5d0bbb7f0a5516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parametrization_second_best",
   "id": "480c28c84a92f392",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To get the reference points available\n",
    "# production_data_available = production_df.groupby(['main_id', 'facility_group_id'], dropna=False).agg(\n",
    "#     commodities=('commodity', lambda x: x.unique().tolist()),\n",
    "#     reference_points=('reference_point', lambda x: x.unique().tolist()),\n",
    "#     material_types=('material_type', lambda x: x.unique().tolist())\n",
    "# ).reset_index()\n"
   ],
   "id": "20f35a3ee65057a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clean technosphere table and harmonize units",
   "id": "497c59d05702601f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "technosphere_df_best = get_info_for_ids(technosphere_table, best_ids)\n",
    "biosphere_df_best = get_info_for_ids(env_table, best_ids)"
   ],
   "id": "bd34e63e826994ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "technosphere_df_third = get_info_for_ids(technosphere_table, third_best_ids)\n",
    "biosphere_df_third = get_info_for_ids(env_table, third_best_ids)"
   ],
   "id": "a37ed10d87de0025",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Separe energy and material\n",
    "energy_df_best = technosphere_df_best[technosphere_df_best['flow_type'] == 'Energy']\n",
    "material_df_best = technosphere_df_best[technosphere_df_best['flow_type'] == 'Material use']"
   ],
   "id": "98a8e9ffb9c80318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "energy_df_third = technosphere_df_third[technosphere_df_third['flow_type'] == 'Energy']\n",
    "material_df_third = technosphere_df_third[technosphere_df_third['flow_type'] == 'Material use']"
   ],
   "id": "8c9438125fae5e32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "energy_table = technosphere_table[technosphere_table['flow_type'] == 'Energy']",
   "id": "2899cdf75c3316c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Deal with energy flows",
   "id": "d9b8c083aa5daf34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## New version\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Direct energy units → MJ ---\n",
    "UNIT_TO_MJ = {\n",
    "    'mj':   1.0,\n",
    "    'gj':   1_000.0,\n",
    "    'tj':   1_000_000.0,\n",
    "    'j':    1e-6,\n",
    "    'wh':   0.0036,\n",
    "    'kwh':  3.6,\n",
    "    'mwh':  3_600.0,\n",
    "    'gwh':  3_600_000.0,\n",
    "}\n",
    "\n",
    "# --- Volume unit multipliers (to liters) ---\n",
    "VOLUME_TO_L = {\n",
    "    'l': 1.0, 'liter': 1.0, 'litre': 1.0, 'liters': 1.0, 'litres': 1.0,\n",
    "    'kl': 1_000.0, 'kiloliter': 1_000.0, 'kilolitre': 1_000.0,\n",
    "    'ml': 1_000_000.0, 'megaliter': 1_000_000.0, 'megalitre': 1_000_000.0,\n",
    "    'gallon': 3.78541, 'gallons': 3.78541,\n",
    "}\n",
    "\n",
    "CUBIC_M_TO_M3 = {'m3': 1.0, 'm^3': 1.0, 'cubicmeter': 1.0, 'cubicmeters': 1.0}\n",
    "\n",
    "# --- Default LHVs (edit with site/company data whenever you can) ---\n",
    "DEFAULT_LHV = {\n",
    "    'diesel':      {'MJ/kg': 43.0, 'MJ/L': 38.6, 'density_kg_per_L': 0.835},\n",
    "    'gasoline':    {'MJ/kg': 44.0, 'MJ/L': 34.2, 'density_kg_per_L': 0.745},\n",
    "    'heavy_fuel_oil': {'MJ/kg': 40.5, 'MJ/L': 39.69, 'density_kg_per_L': 0.98},\n",
    "    'coal':        {'MJ/kg': 25.0},\n",
    "    'natural_gas': {'MJ/m3': 38.0, 'MJ/L': 22.5, 'density_kg_per_L': 0.7},\n",
    "    'propane':     {'MJ/kg': 46.4, 'MJ/L': 25.3, 'density_kg_per_L': 0.493},\n",
    "    'electricity': {'MJ/kWh': 3.6},\n",
    "    'explosives':  {'MJ/kg': 4.0},\n",
    "    'coke':        {'MJ/kg': 28.0},\n",
    "    'wood':        {'MJ/kg': 16.0},\n",
    "}\n",
    "\n",
    "\n",
    "# --- Subflow canonicalization (aliases + strip pipe suffixes) ---\n",
    "SUBFLOW_ALIASES = {\n",
    "    'petrol': 'gasoline',\n",
    "    'heavy fuel oil': 'heavy_fuel_oil',\n",
    "    'hfo': 'heavy_fuel_oil',\n",
    "    'natural gas': 'natural_gas',\n",
    "    'explosive': 'explosives',\n",
    "    'lpg': 'propane',\n",
    "    'surface/underground_emulsion_&_anfo': 'explosives',\n",
    "    'surface/undergound_emulsion_&_anfo': 'explosives', # to avoid error\n",
    "}\n",
    "\n",
    "def _norm_unit(x):\n",
    "    if pd.isna(x): return None\n",
    "    return str(x).strip().lower().replace(' ', '')\n",
    "\n",
    "def _canon_subflow(x):\n",
    "    if pd.isna(x): return None\n",
    "    s = str(x).strip().lower()\n",
    "    if '|' in s:\n",
    "        s = s.split('|', 1)[0].strip()\n",
    "    s = SUBFLOW_ALIASES.get(s, s)\n",
    "    s_us = s.replace(' ', '_')\n",
    "    return s_us\n",
    "\n",
    "def standardize_energy_to_MJ(\n",
    "    df,\n",
    "    subflow_col='subflow_type',\n",
    "    unit_col='unit',\n",
    "    value_col='value',\n",
    "    lhv_table=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert energy/fuel rows to MJ.\n",
    "    \"\"\"\n",
    "    lhv = (lhv_table or DEFAULT_LHV).copy()\n",
    "    out = df.copy()\n",
    "\n",
    "    # Normalize\n",
    "    out['_unit_n'] = out[unit_col].map(_norm_unit)\n",
    "    out['_subflow_n'] = out[subflow_col].map(_canon_subflow)\n",
    "    out[value_col] = pd.to_numeric(out[value_col], errors='coerce')\n",
    "\n",
    "    # 1) Direct energy units\n",
    "    direct_mask = out['_unit_n'].isin(UNIT_TO_MJ)\n",
    "    out.loc[direct_mask, 'value_MJ'] = (\n",
    "        out.loc[direct_mask, value_col] *\n",
    "        out.loc[direct_mask, '_unit_n'].map(UNIT_TO_MJ)\n",
    "    )\n",
    "    out.loc[direct_mask, 'unit_source'] = 'direct_unit'\n",
    "    out.loc[direct_mask, 'assumption_note'] = (\n",
    "        out.loc[direct_mask, '_unit_n'].map(lambda u: f\"{u}→MJ factor={UNIT_TO_MJ[u]}\")\n",
    "    )\n",
    "\n",
    "    # 2) Fuels via LHV\n",
    "    fuel_rows = ~direct_mask & out['_subflow_n'].notna() & out[value_col].notna()\n",
    "    for idx in out.index[fuel_rows]:\n",
    "        sub = out.at[idx, '_subflow_n']\n",
    "        unit = out.at[idx, '_unit_n']\n",
    "        val  = out.at[idx, value_col]\n",
    "        lhv_data = lhv.get(sub)\n",
    "\n",
    "        if not lhv_data:\n",
    "            out.at[idx, 'unit_source'] = 'missing_factor'\n",
    "            out.at[idx, 'assumption_note'] = f\"No LHV for subflow={sub}\"\n",
    "            continue\n",
    "\n",
    "        converted = False\n",
    "\n",
    "        # A) Mass units (kg, t, lbs)\n",
    "        if unit in ('kg', 'kilogram', 'kilograms', 't', 'tonne', 'tonnes',\n",
    "                    'metricton', 'ton', 'lb', 'lbs', 'pound', 'pounds'):\n",
    "            # Determine multiplier to convert mass unit to kg\n",
    "            mult_kg = 1.0\n",
    "            if unit.startswith('t'):\n",
    "                mult_kg = 1000.0\n",
    "            elif unit in ('lb', 'lbs', 'pound', 'pounds'):\n",
    "                mult_kg = 0.453592 # lbs to kg\n",
    "\n",
    "            factor_kg = lhv_data.get('MJ/kg')\n",
    "            if factor_kg:\n",
    "                out.at[idx, 'value_MJ'] = val * mult_kg * factor_kg\n",
    "                out.at[idx, 'unit_source'] = 'lhv_factor'\n",
    "                out.at[idx, 'assumption_note'] = f\"{sub} MJ/kg={factor_kg} (from {unit})\"\n",
    "                converted = True\n",
    "\n",
    "        # B) Volume units (L, kL, ML, Gallons)\n",
    "        elif unit in VOLUME_TO_L:\n",
    "            mult_L = VOLUME_TO_L[unit]\n",
    "            factor_l = lhv_data.get('MJ/L')\n",
    "            if factor_l is None and lhv_data.get('density_kg_per_L') and lhv_data.get('MJ/kg'):\n",
    "                dens = lhv_data.get('density_kg_per_L')\n",
    "                factor_kg = lhv_data.get('MJ/kg')\n",
    "                mass_kg = val * mult_L * dens\n",
    "                out.at[idx, 'value_MJ'] = mass_kg * factor_kg\n",
    "                out.at[idx, 'unit_source'] = 'lhv+density'\n",
    "                out.at[idx, 'assumption_note'] = f\"{sub} L→kg via {dens} kg/L; MJ/kg={factor_kg}\"\n",
    "                converted = True\n",
    "            elif factor_l:\n",
    "                out.at[idx, 'value_MJ'] = val * mult_L * factor_l\n",
    "                out.at[idx, 'unit_source'] = 'lhv_factor'\n",
    "                out.at[idx, 'assumption_note'] = f\"{sub} MJ/L={factor_l}\"\n",
    "                converted = True\n",
    "\n",
    "        # C) Volume units (m3)\n",
    "        elif unit in CUBIC_M_TO_M3:\n",
    "            factor_m3 = lhv_data.get('MJ/m3')\n",
    "            if factor_m3:\n",
    "                out.at[idx, 'value_MJ'] = val * CUBIC_M_TO_M3[unit] * factor_m3\n",
    "                out.at[idx, 'unit_source'] = 'lhv_factor'\n",
    "                out.at[idx, 'assumption_note'] = f\"{sub} MJ/m3={factor_m3}\"\n",
    "                converted = True\n",
    "\n",
    "        if not converted:\n",
    "            out.at[idx, 'unit_source'] = 'missing_factor'\n",
    "            out.at[idx, 'assumption_note'] = f\"No conversion rule for subflow={sub}, unit={unit}\"\n",
    "\n",
    "    # Final flags\n",
    "    out['unit_standard'] = np.where(out['value_MJ'].notna(), 'MJ', None)\n",
    "    out['needs_factor'] = out['value_MJ'].isna() & out[value_col].notna()\n",
    "    out = out.drop(columns=['_unit_n', '_subflow_n'], errors='ignore')\n",
    "    return out"
   ],
   "id": "435b31c76a09c2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "energy_df_best_mj = standardize_energy_to_MJ(energy_df_best,\n",
    "                                    subflow_col='subflow_type',\n",
    "                                    unit_col='unit',\n",
    "                                    value_col='value',\n",
    "                                    lhv_table=None)   # or pass a custom dict"
   ],
   "id": "3c09757cfb88baf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "energy_df_third_mj = standardize_energy_to_MJ(energy_df_third,\n",
    "                                    subflow_col='subflow_type',\n",
    "                                    unit_col='unit',\n",
    "                                    value_col='value',\n",
    "                                    lhv_table=None)"
   ],
   "id": "8ab5cc988e3cf70a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Deal with material flows",
   "id": "c12e9302a60a627"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "material_table = technosphere_table[technosphere_table['flow_type'] == 'Material use']",
   "id": "4a2cbfce59f9c145",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# liters prefixes → L\n",
    "VOLUME_TO_L = {\n",
    "    'l': 1.0, 'liter': 1.0, 'litre': 1.0, 'liters': 1.0, 'litres': 1.0,\n",
    "    'kl': 1_000.0, 'kiloliter': 1_000.0, 'kilolitre': 1_000.0,\n",
    "    'ml': 1_000_000.0, 'megaliter': 1_000_000.0, 'megalitre': 1_000_000.0,\n",
    "}\n",
    "\n",
    "# Default densities (kg/L) – please override with site-specific values when you have them\n",
    "DEFAULT_DENSITY = {\n",
    "    # Oils & lubricants family\n",
    "    'lubricants': 0.88,\n",
    "    'hydraulic oil': 0.88,\n",
    "    'transmission oil': 0.88,\n",
    "    'motor oil': 0.88,\n",
    "    'drill oil': 0.88,\n",
    "    'compressor oil': 0.88,\n",
    "\n",
    "    # Acids (typical commercial concentrations)\n",
    "    'sulfuric acid (h2so4)': 1.84,    # ~98%\n",
    "    'hydrochloric acid (hcl)': 1.19,  # ~37%\n",
    "    'nitric acid (hno3)': 1.51,       # ~68–70%\n",
    "\n",
    "    # If you have aqueous reagents (e.g., “sodium cyanide solution”) add their conc/density here.\n",
    "}\n",
    "\n",
    "# Canonicalize names (left part before '|', lowercased)\n",
    "ALIASES = {\n",
    "    'petrol': 'gasoline',\n",
    "    'grindingmedia': 'grinding media',\n",
    "    '3/4\\'\\'balls': 'grinding media',\n",
    "    '2\\'\\'balls': 'grinding media',\n",
    "    '2.5\\'\\'balls': 'grinding media',\n",
    "    '5.5\\'\\'balls': 'grinding media',\n",
    "    'polyfrothh57': 'polyfroth h57',\n",
    "    'antiscalant': 'anti-scalant',\n",
    "}\n",
    "\n",
    "def _norm_text(x):\n",
    "    if pd.isna(x): return None\n",
    "    return str(x).strip()\n",
    "\n",
    "def _canon_subflow(s):\n",
    "    if s is None: return None\n",
    "    # take leftmost token before a pipe and lowercase\n",
    "    base = s.split('|', 1)[0].strip().lower()\n",
    "    # strip extra spaces and collapse doubles\n",
    "    base = ' '.join(base.split())\n",
    "    return ALIASES.get(base.replace(' ', ''), base)\n",
    "\n",
    "def standardize_materials_to_t(df, subflow_col='subflow_type', unit_col='unit', value_col='value',\n",
    "                               density_table=None):\n",
    "    \"\"\"\n",
    "    Convert 'material' rows to tonnes.\n",
    "    Adds:\n",
    "      - mass_t : numeric mass in tonnes\n",
    "      - mass_source : 't','kg→t','L×density→t','missing_density','unknown_unit'\n",
    "      - mass_note : short note on the assumption used\n",
    "      - needs_density : True when a volume row had no density mapping\n",
    "    \"\"\"\n",
    "    den = {k.lower(): v for k, v in (density_table or DEFAULT_DENSITY).items()}\n",
    "    out = df.copy()\n",
    "\n",
    "    out['_unit_n'] = out[unit_col].astype(str).str.strip().str.lower().str.replace(' ', '', regex=False)\n",
    "    out['_subflow_n'] = out[subflow_col].map(_canon_subflow)\n",
    "    out[value_col] = pd.to_numeric(out[value_col], errors='coerce')\n",
    "\n",
    "    # direct tonnes\n",
    "    mask_t = out['_unit_n'].isin({'t','tonne','tonnes','metricton','ton'})\n",
    "    out.loc[mask_t, 'mass_t'] = out.loc[mask_t, value_col].astype(float)\n",
    "    out.loc[mask_t, 'mass_source'] = 't'\n",
    "    out.loc[mask_t, 'mass_note'] = 'reported in tonnes'\n",
    "\n",
    "    # kg → t\n",
    "    mask_kg = out['_unit_n'].isin({'kg','kilogram','kilograms'})\n",
    "    out.loc[mask_kg, 'mass_t'] = out.loc[mask_kg, value_col] / 1000.0\n",
    "    out.loc[mask_kg, 'mass_source'] = 'kg→t'\n",
    "    out.loc[mask_kg, 'mass_note'] = 'kg/1000'\n",
    "\n",
    "    # liters family → t using density (kg/L)\n",
    "    mask_L = out['_unit_n'].isin(VOLUME_TO_L)\n",
    "    if mask_L.any():\n",
    "        multL = out.loc[mask_L, '_unit_n'].map(VOLUME_TO_L)\n",
    "        # find density per row from mapping on canonical subflow\n",
    "        dens = out.loc[mask_L, '_subflow_n'].map(lambda s: den.get(s if s else '', np.nan))\n",
    "        mass_t = (out.loc[mask_L, value_col] * multL * dens) / 1000.0\n",
    "        out.loc[mask_L, 'mass_t'] = mass_t\n",
    "        out.loc[mask_L, 'mass_source'] = np.where(dens.notna(), 'L×density→t', 'missing_density')\n",
    "        out.loc[mask_L, 'mass_note'] = np.where(\n",
    "            dens.notna(),\n",
    "            (out.loc[mask_L, '_unit_n'].map(str) + f\"→L × density kg/L; density=\" + dens.map(lambda x: f\"{x:g}\")),\n",
    "            \"volume reported; no density mapping for this subflow\"\n",
    "        )\n",
    "\n",
    "    # mark unknown units\n",
    "    mask_done = mask_t | mask_kg | mask_L\n",
    "    out.loc[~mask_done & out[value_col].notna(), 'mass_source'] = 'unknown_unit'\n",
    "    out.loc[~mask_done & out[value_col].notna(), 'mass_note'] = 'no rule for this unit'\n",
    "\n",
    "    out['needs_density'] = (out['mass_source'] == 'missing_density')\n",
    "\n",
    "    # clean temp\n",
    "    out = out.drop(columns=['_unit_n','_subflow_n'])\n",
    "    return out\n"
   ],
   "id": "78c7c7c8c29e1315",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "material_df_best_t = standardize_materials_to_t(material_df_best)\n",
    "material_df_third_t = standardize_materials_to_t(material_df_third)"
   ],
   "id": "9803f3ba509b4cb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Normalize technosphere and biosphere flows by production values",
   "id": "cc46872535b0200a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop rows where needs_factor is True\n",
    "energy_df_third_mj.drop(energy_df_third_mj[energy_df_third_mj['needs_factor']].index, inplace=True)"
   ],
   "id": "e4820a4a6c1347ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "def normalize_by_production(df, production_df, value_col='value', prod_col='value_tonnes', prod_agg='sum'):\n",
    "    df = df.copy()\n",
    "    # ensure numeric\n",
    "    df[value_col] = pd.to_numeric(df[value_col], errors='coerce')\n",
    "    prod = production_df.copy()\n",
    "    prod[prod_col] = pd.to_numeric(prod[prod_col], errors='coerce')\n",
    "\n",
    "    # aggregate to unique per key\n",
    "    main_prod = (prod.dropna(subset=['main_id'])\n",
    "                    .groupby('main_id', as_index=False)[prod_col]\n",
    "                    .agg(prod_agg)\n",
    "                    .rename(columns={prod_col: 'value_tonnes_main'}))\n",
    "    group_prod = (prod.dropna(subset=['facility_group_id'])\n",
    "                     .groupby('facility_group_id', as_index=False)[prod_col]\n",
    "                     .agg(prod_agg)\n",
    "                     .rename(columns={prod_col: 'value_tonnes_group'}))\n",
    "\n",
    "    # safe 1:1 merges\n",
    "    out = df.merge(main_prod, on='main_id', how='left').merge(group_prod, on='facility_group_id', how='left')\n",
    "\n",
    "    # prefer main_id match, fallback to facility_group_id\n",
    "    out['value_tonnes_match'] = out['value_tonnes_main'].combine_first(out['value_tonnes_group'])\n",
    "    out['value_normalized'] = out[value_col] / out['value_tonnes_match']\n",
    "\n",
    "    # diagnostics\n",
    "    out['normalization_key'] = None\n",
    "    out.loc[out['value_tonnes_main'].notna(), 'normalization_key'] = 'main_id'\n",
    "    out.loc[out['value_tonnes_main'].isna() & out['value_tonnes_group'].notna(), 'normalization_key'] = 'facility_group_id'\n",
    "    return out"
   ],
   "id": "817e79f1c459fbae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "energy_df_third_mj_norm = normalize_by_production(energy_df_third_mj, production_df_third, value_col='value_MJ')",
   "id": "8c63732d8747f2d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "material_df_third_norm = normalize_by_production(material_df_third_t, production_df_third, value_col='mass_t', prod_col='value_tonnes', prod_agg='sum')",
   "id": "973ab05164b87761",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "material_df_third_norm",
   "id": "e624105cdf7144e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "subflow_type_agg = {\n",
    "    # electricity\n",
    "    'Electricity consumption|Generated on-site': 'Electricity',\n",
    "    'Electricity consumption': 'Electricity',\n",
    "    'Electricity consumption|Grid electricity': 'Electricity',\n",
    "    'Electricity consumption|Not specified': 'Electricity',\n",
    "    'Electricity consumption|Non-renewable electricity use': 'Electricity',\n",
    "    'Solar': 'Electricity',\n",
    "\n",
    "    # Fuels\n",
    "    'Diesel': 'Diesel',\n",
    "    'Diesel|Mobile equipment': 'Diesel',\n",
    "    'Diesel|Stationary equipment': 'Diesel',\n",
    "    'Gasoline': 'Gasoline',\n",
    "    'Gasoline|Mobile equipment': 'Gasoline',\n",
    "    'Petrol': 'Gasoline',\n",
    "    'Oil': 'Gasoline',  # usually refers to gasoline\n",
    "    'Light Fuel & Gasoline': 'Gasoline',\n",
    "    'Lubricating Oils & Greases': 'Lubricants',\n",
    "    'Biodiesel': 'Diesel',\n",
    "    'Propane': 'LPG-Propane',\n",
    "    'LPG': 'LPG-Propane',\n",
    "    'LPG|Mobile equipment': 'LPG-Propane',\n",
    "    'LPG|Stationary equipment': 'LPG-Propane',\n",
    "    'Acetylene': 'LPG-Propane',\n",
    "    'Natural gas': 'Natural gas',\n",
    "    'Naphta': 'Naphtha',  # spelling\n",
    "    'Aviation fuel': 'Aviation fuel',\n",
    "    'Non-renewable fuel use': 'Diesel',\n",
    "\n",
    "    # Explosives\n",
    "    'Explosives': 'Explosives',\n",
    "    'Total blasting agents used e.g. anfo': 'Explosives',\n",
    "    'ANFO': 'Explosives',\n",
    "    'Emulsion ANFO': 'Explosives',\n",
    "    'Emulsions': 'Explosives',\n",
    "    'Emulsion': 'Explosives',\n",
    "    'Dynamite': 'Explosives',\n",
    "    'Ammonium nitrate': 'Explosives',  # (treat as energy only if you ANFO-equivalize)\n",
    "\n",
    "    # Others\n",
    "    'Used oil': 'Other',   # usually MATERIAL (lubricants); map to energy only if burned\n",
    "    'Other': 'Other',\n",
    "}"
   ],
   "id": "28150949197abf2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add a subflow_type_agg column to the energy_std_norm DataFrame based on the dictionnary\n",
    "energy_df_third_mj_norm['subflow_type_agg'] = energy_df_third_mj_norm['subflow_type'].map(subflow_type_agg).fillna(energy_df_third_mj_norm['subflow_type'])"
   ],
   "id": "25001bc7098cf34f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group by subflow_type_agg and normalization_key, aggregating the normalized values\n",
    "energy_df_third_mj_norm_agg = (\n",
    "    energy_df_third_mj_norm\n",
    "    .groupby(\n",
    "        [\n",
    "            'main_id', 'facility_group_id', 'company_id',\n",
    "            'year', 'flow_type', 'subflow_type_agg'        ],\n",
    "        dropna=False, as_index=False\n",
    "    )\n",
    "    .agg(value_normalized_sum=('value_normalized', 'sum'))\n",
    ")"
   ],
   "id": "d7a111ae9a4e89c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# First plots",
   "id": "195747da5940c3d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "energy_plot_df = merge_without_suffixes(energy_df_third_mj_norm_agg, archetypes_third_best, keys=(\"main_id\", \"facility_group_id\"), how=\"left\")",
   "id": "b6fe0305fb2adb21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "energy_plot_df = merge_without_suffixes(energy_plot_df, main_third_best_df, keys=(\"main_id\", \"facility_group_id\"), how=\"left\")",
   "id": "f6baa9928cd9ab28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "energy_plot_df['commodities'].unique()",
   "id": "e41dfe73de56827e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "material_plot_df = merge_without_suffixes(material_df_third, archetypes_best, keys=(\"main_id\", \"facility_group_id\"), how=\"left\")",
   "id": "e73c428ace2588c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def stripplot_with_barplot(\n",
    "    data, x, y, hue=None, style=None, log_scale=False,\n",
    "    outfile=None, dpi=600\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a scatterplot with overlaid barplot for mean/median.\n",
    "    Optionally save as high-resolution figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Scatterplot (replaces stripplot)\n",
    "    sns.scatterplot(\n",
    "        data=data,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=hue,\n",
    "        style=style,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Barplot for mean/median\n",
    "    sns.barplot(\n",
    "        data=data,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=hue,\n",
    "        estimator='median',\n",
    "        errorbar=None,\n",
    "        alpha=0.3,\n",
    "        width=0.6,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Add vertical separators between x categories\n",
    "    for i in range(len(data[x].unique()) - 1):\n",
    "        ax.axvline(i + 0.5, color=\"grey\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    # Log scale if requested\n",
    "    if log_scale:\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.set_title(\n",
    "        f\"Plot: {y} by {x}\" +\n",
    "        (f\" (colored by {hue})\" if hue else \"\") +\n",
    "        (f\" (styled by {style})\" if style else \"\")\n",
    "    )\n",
    "    ax.set_xlabel(x)\n",
    "    ax.set_ylabel(y)\n",
    "\n",
    "    if hue or style:\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else:\n",
    "        ax.legend_.remove()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save if requested\n",
    "    if outfile:\n",
    "        fig.savefig(outfile, dpi=dpi, bbox_inches=\"tight\", format=\"pdf\")\n",
    "        print(f\"Saved figure to {outfile} (dpi={dpi})\")\n",
    "\n",
    "    plt.show()"
   ],
   "id": "5b25666a98fd867d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stripplot_energy_df = stripplot_with_barplot(\n",
    "    energy_plot_df,\n",
    "    \"subflow_type_agg\",\n",
    "    \"value_normalized_sum\",\n",
    "    hue=\"mining_method\",\n",
    "    style='commodities',\n",
    "    log_scale=True,\n",
    "    outfile=\"results/stripplot_energy_third_method.pdf\",\n",
    "    dpi=600\n",
    ")"
   ],
   "id": "35b60bb06563204c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stripplot_material_df = stripplot_with_barplot(\n",
    "    material_plot_df,\n",
    "    \"subflow_type\",\n",
    "    \"mass_t\",\n",
    "    hue=\"mining_submethod\",\n",
    "    log_scale=True,\n",
    "    outfile=\"results/stripplot_material_third_submethod.pdf\",\n",
    "    dpi=600\n",
    ")"
   ],
   "id": "a4e4dd376ba7cb3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Build relationships for parametrization",
   "id": "a66b7a60b008d346"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------\n",
    "# Core fitting utilities\n",
    "# ---------------------------\n",
    "\n",
    "def _safe_loglik(x, rv):\n",
    "    \"\"\"Return finite log-likelihood sum for fitted frozen rv.\"\"\"\n",
    "    logpdf = rv.logpdf(x)\n",
    "    logpdf = logpdf[np.isfinite(logpdf)]\n",
    "    return float(np.sum(logpdf)) if logpdf.size else -np.inf\n",
    "\n",
    "def _eval_fit(name, rv, x, k):\n",
    "    \"\"\"Compute LL, AIC, BIC, KS p for a frozen rv with k free parameters.\"\"\"\n",
    "    n = len(x)\n",
    "    ll = _safe_loglik(x, rv)\n",
    "    aic = 2 * k - 2 * ll\n",
    "    bic = k * np.log(n) - 2 * ll\n",
    "    # KS test using the fitted CDF\n",
    "    ks_stat, ks_p = stats.kstest(x, rv.cdf)\n",
    "    return dict(dist=name, ll=ll, aic=aic, bic=bic, ks_p=ks_p, rv=rv, k=k)\n",
    "\n",
    "def _fit_candidates(x, candidates=(\"lognormal\", \"gamma\", \"weibull_min\", \"normal\")):\n",
    "    \"\"\"Fit candidate distributions on strictly positive x.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    results = []\n",
    "\n",
    "    # LOGNORMAL: s (sigma), scale=exp(mu), loc fixed 0 => k=2\n",
    "    if \"lognormal\" in candidates:\n",
    "        try:\n",
    "            s, loc, scale = stats.lognorm.fit(x, floc=0)\n",
    "            rv = stats.lognorm(s=s, loc=0, scale=scale)\n",
    "            results.append(_eval_fit(\"lognormal\", rv, x, k=2))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # GAMMA: shape a, scale theta, loc fixed 0 => k=2\n",
    "    if \"gamma\" in candidates:\n",
    "        try:\n",
    "            a, loc, scale = stats.gamma.fit(x, floc=0)\n",
    "            rv = stats.gamma(a=a, loc=0, scale=scale)\n",
    "            results.append(_eval_fit(\"gamma\", rv, x, k=2))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # WEIBULL(min): c (shape), scale, loc fixed 0 => k=2\n",
    "    if \"weibull_min\" in candidates:\n",
    "        try:\n",
    "            c, loc, scale = stats.weibull_min.fit(x, floc=0)\n",
    "            rv = stats.weibull_min(c=c, loc=0, scale=scale)\n",
    "            results.append(_eval_fit(\"weibull\", rv, x, k=2))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # NORMAL: mu, sigma => k=2\n",
    "    if \"normal\" in candidates:\n",
    "        try:\n",
    "            mu, sigma = stats.norm.fit(x)\n",
    "            rv = stats.norm(loc=mu, scale=max(sigma, 1e-12))\n",
    "            results.append(_eval_fit(\"normal\", rv, x, k=2))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Sort by BIC then AIC\n",
    "    results.sort(key=lambda d: (d[\"bic\"], d[\"aic\"]))\n",
    "    return results\n",
    "\n",
    "def _readable_params(best):\n",
    "    \"\"\"Human-readable parameter summary by distribution.\"\"\"\n",
    "    rv = best[\"rv\"]\n",
    "    name = best[\"dist\"]\n",
    "    # scipy frozen RV stores parameters in args/kwds\n",
    "    args = getattr(rv, \"args\", ())\n",
    "    kw = getattr(rv, \"kwds\", {})\n",
    "\n",
    "    if name == \"lognormal\":\n",
    "        # s = sigma, scale = exp(mu)\n",
    "        s = kw.get(\"s\", args[0] if args else None)\n",
    "        scale = kw.get(\"scale\", args[-1] if args else None)\n",
    "        mu = np.log(scale) if scale is not None else np.nan\n",
    "        return dict(mu_log=mu, sigma_log=s, scale=scale)\n",
    "    elif name == \"gamma\":\n",
    "        a = kw.get(\"a\", args[0] if args else None)\n",
    "        scale = kw.get(\"scale\", args[-1] if args else None)\n",
    "        return dict(shape=a, scale=scale)\n",
    "    elif name == \"weibull\":\n",
    "        c = kw.get(\"c\", args[0] if args else None)\n",
    "        scale = kw.get(\"scale\", args[-1] if args else None)\n",
    "        return dict(shape=c, scale=scale)\n",
    "    elif name == \"normal\":\n",
    "        loc = kw.get(\"loc\", 0.0)\n",
    "        scale = kw.get(\"scale\", np.nan)\n",
    "        return dict(mu=loc, sigma=scale)\n",
    "    return dict()\n",
    "\n",
    "# ---------------------------\n",
    "# Public API\n",
    "# ---------------------------\n",
    "\n",
    "def fit_parametric_by_group(\n",
    "    df,\n",
    "    value_col=\"value_normalized_sum\",\n",
    "    group_cols=(\"subflow_type_agg\", \"mining_method\"),\n",
    "    min_n=6,\n",
    "    winsorize_pct=None,  # e.g., (0.01, 0.01) for 1% tails\n",
    "    candidates=(\"lognormal\", \"gamma\", \"weibull_min\", \"normal\"),\n",
    "    results_csv=None,\n",
    "    pdf_path=None,\n",
    "    pdf_log_x=True,\n",
    "    pdf_log_y=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit parametric distributions to positive data by group.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_df : DataFrame with:\n",
    "        group columns, n, min, max, p05, p50, p95,\n",
    "        best_dist, AIC, BIC, KS_p, params_json (readable dict)\n",
    "    Side effects (optional)\n",
    "    -----------------------\n",
    "    - results_csv: write CSV\n",
    "    - pdf_path: write a multi-page PDF with histogram + best-fit PDF\n",
    "    \"\"\"\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols = [group_cols]\n",
    "\n",
    "    # Work copy and basic hygiene\n",
    "    data = df.copy()\n",
    "    if value_col not in data.columns:\n",
    "        raise ValueError(f\"Missing column '{value_col}'\")\n",
    "    for gc in group_cols:\n",
    "        if gc not in data.columns:\n",
    "            raise ValueError(f\"Missing group column '{gc}'\")\n",
    "\n",
    "    # Positive values only\n",
    "    data = data[np.isfinite(data[value_col]) & (data[value_col] > 0)].copy()\n",
    "\n",
    "    # Optional winsorization (by group)\n",
    "    if winsorize_pct is not None:\n",
    "        low_p, high_p = winsorize_pct\n",
    "        def _wins_g(g):\n",
    "            lo = g[value_col].quantile(low_p) if low_p else g[value_col].min()\n",
    "            hi = g[value_col].quantile(1 - high_p) if high_p else g[value_col].max()\n",
    "            g[value_col] = g[value_col].clip(lo, hi)\n",
    "            return g\n",
    "        data = data.groupby(list(group_cols), group_keys=False).apply(_wins_g)\n",
    "\n",
    "    records = []\n",
    "    plot_payload = []  # (group_key, x, best_fit)\n",
    "\n",
    "    for gkey, g in data.groupby(list(group_cols)):\n",
    "        x = g[value_col].dropna().values\n",
    "        n = len(x)\n",
    "\n",
    "        # Summary stats for LCA Algebraic bounds\n",
    "        if n == 0:\n",
    "            rec = {gc: gv for gc, gv in zip(group_cols, gkey)}\n",
    "            rec.update(dict(n=0, min=np.nan, max=np.nan, p05=np.nan, p50=np.nan, p95=np.nan,\n",
    "                            best_dist=\"no_data\", AIC=np.nan, BIC=np.nan, KS_p=np.nan, params_json={}))\n",
    "            records.append(rec)\n",
    "            continue\n",
    "\n",
    "        p05, p50, p95 = np.percentile(x, [5, 50, 95])\n",
    "        xmin, xmax = float(np.min(x)), float(np.max(x))\n",
    "\n",
    "        if n < min_n:\n",
    "            rec = {gc: gv for gc, gv in zip(group_cols, gkey)}\n",
    "            rec.update(dict(n=n, min=xmin, max=xmax, p05=p05, p50=p50, p95=p95,\n",
    "                            best_dist=\"insufficient_data\", AIC=np.nan, BIC=np.nan, KS_p=np.nan, params_json={}))\n",
    "            records.append(rec)\n",
    "            continue\n",
    "\n",
    "        fits = _fit_candidates(x, candidates=candidates)\n",
    "        if not fits:\n",
    "            rec = {gc: gv for gc, gv in zip(group_cols, gkey)}\n",
    "            rec.update(dict(n=n, min=xmin, max=xmax, p05=p05, p50=p50, p95=p95,\n",
    "                            best_dist=\"fit_failed\", AIC=np.nan, BIC=np.nan, KS_p=np.nan, params_json={}))\n",
    "            records.append(rec)\n",
    "            continue\n",
    "\n",
    "        best = fits[0]\n",
    "        params_dict = _readable_params(best)\n",
    "\n",
    "        rec = {gc: gv for gc, gv in zip(group_cols, gkey)}\n",
    "        rec.update(dict(\n",
    "            n=n, min=xmin, max=xmax, p05=p05, p50=p50, p95=p95,\n",
    "            best_dist=best[\"dist\"], AIC=best[\"aic\"], BIC=best[\"bic\"], KS_p=best[\"ks_p\"],\n",
    "            params_json=params_dict\n",
    "        ))\n",
    "        records.append(rec)\n",
    "        plot_payload.append((gkey, x, best))\n",
    "\n",
    "    results_df = pd.DataFrame.from_records(records).sort_values(list(group_cols) + [\"best_dist\"])\n",
    "\n",
    "    if results_csv:\n",
    "        Path(results_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "        results_df.to_csv(results_csv, index=False)\n",
    "\n",
    "    # Multipage PDF (one figure per page; journal-friendly)\n",
    "    if pdf_path:\n",
    "        Path(pdf_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with PdfPages(pdf_path) as pdf:\n",
    "            for gkey, x, best in plot_payload:\n",
    "                fig, ax = plt.subplots(figsize=(7, 5))\n",
    "                # Histogram\n",
    "                n_bins = max(8, int(np.sqrt(len(x))))\n",
    "                ax.hist(x, bins=n_bins, density=True)\n",
    "\n",
    "                # Best-fit PDF\n",
    "                rv = best[\"rv\"]\n",
    "                xmin = max(1e-12, np.min(x) * 0.8)\n",
    "                xmax = np.max(x) * 1.2\n",
    "                xs = np.linspace(xmin, xmax, 400)\n",
    "                ax.plot(xs, rv.pdf(xs))\n",
    "\n",
    "                if pdf_log_x:\n",
    "                    ax.set_xscale(\"log\")\n",
    "                if pdf_log_y:\n",
    "                    ax.set_yscale(\"log\")\n",
    "\n",
    "                title = \" | \".join([f\"{gc}={gv}\" for gc, gv in zip(group_cols, gkey)])\n",
    "                ax.set_title(f\"{title}\\nBest: {best['dist'].upper()}  (n={len(x)}, BIC={best['bic']:.1f})\")\n",
    "                ax.set_xlabel(value_col)\n",
    "                ax.set_ylabel(\"Density\")\n",
    "                fig.tight_layout()\n",
    "                pdf.savefig(fig, dpi=600)  # 600 dpi pages\n",
    "                plt.close(fig)\n",
    "\n",
    "    return results_df\n"
   ],
   "id": "38b59e232b3ce39d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = fit_parametric_by_group(\n",
    "    df=energy_plot_df,\n",
    "    value_col=\"value_normalized_sum\",\n",
    "    group_cols=(\"subflow_type_agg\", \"mining_method\"),\n",
    "    min_n=3,  # minimum number of samples per group\n",
    "    winsorize_pct=(0.01, 0.01),  # set None to disable\n",
    "    candidates=(\"lognormal\", \"gamma\", \"weibull_min\", \"normal\"),\n",
    "    results_csv=\"results/best_fit_distributions.csv\",\n",
    "    pdf_path=\"results/subflow_archetype_fit_plots.pdf\",\n",
    "    pdf_log_x=True,\n",
    "    pdf_log_y=False\n",
    ")"
   ],
   "id": "f37176484412afd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results",
   "id": "6128336255817574",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Grade parametrization",
   "id": "af1a1620f0c2b952"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "au_ids = [\n",
    "'ON-MAIN-687b8c8d',\t#Island\n",
    "'ON-MAIN-7607a50e',\t#Young-Davidson\n",
    "'QC-MAIN-f9e41c2a',\t#Lamaque\n",
    "'GRP-0d911886', #Porcupine #3 sites\n",
    "'GRP-147b3123', #Timmins Operation #2 sites # Also silver in production\n",
    "'GRP-14bfbb82', #Seabee Gold Operation #2 sites\n",
    "'QC-MAIN-9de9bb0d',\t#Kiena\n",
    "'ON-MAIN-c5fefb01',\t#Mishi\n",
    "\n",
    "# Additional ones, only gold in production_df but not in NRCan\n",
    "'BC-MAIN-8eb8be0d', #Red Chris\n",
    "'ON-MAIN-0aadf28f', #Rainy River\n",
    "'ON-MAIN-538513cd', # Hoyle Pond, Part of Porcupine Complex\n",
    "'ON-MAIN-fefeaee4', # Musselwhite\n",
    "'QC-MAIN-02884fb5' #Westwood-Doyon\n",
    "]"
   ],
   "id": "2bcacc560a78e5e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare data",
   "id": "615b08ac57ced088"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Production",
   "id": "b0aa8c6a9dc47bb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the production where either main_id or facility_group_id matches the au_ids qnd reference point is Crude ore\n",
    "production_au = production_df[\n",
    "    (production_df['main_id'].isin(au_ids) | production_df['facility_group_id'].isin(au_ids)) &\n",
    "    (production_df['reference_point'] == 'Crude ore') &\n",
    "    (production_df['prod_id'].str.startswith('PROD'))\n",
    "].copy()"
   ],
   "id": "d5ade9a4eca05d3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "production_au",
   "id": "4c2825e12b21a972",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Energy",
   "id": "7ceff22232642db4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "energy_au = technosphere_table[\n",
    "    (technosphere_table['main_id'].isin(au_ids) | technosphere_table['facility_group_id'].isin(au_ids)) &\n",
    "    (technosphere_table['flow_type'] == 'Energy')\n",
    "].copy()"
   ],
   "id": "c003e4b204e30b1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "energy_au_mj = standardize_energy_to_MJ(energy_au,\n",
    "                                    subflow_col='subflow_type',\n",
    "                                    unit_col='unit',\n",
    "                                    value_col='value',\n",
    "                                    lhv_table=None)"
   ],
   "id": "498b52f40b89ad8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "energy_au_mj.drop(energy_au_mj[energy_au_mj['needs_factor']].index, inplace=True)\n",
   "id": "dd6e2587c9b9ebe8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "energy_au_mj_norm = normalize_by_production(energy_au_mj, production_au, value_col='value_MJ')\n",
   "id": "ca19d38d85811bd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "energy_au_mj_norm",
   "id": "7b64789b05bed735",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add a subflow_type_agg column to the energy_std_norm DataFrame based on the dictionnary\n",
    "energy_au_mj_norm['subflow_type_agg'] = energy_au_mj_norm['subflow_type'].map(subflow_type_agg).fillna(\n",
    "    energy_au_mj_norm['subflow_type'])\n",
    "# Group by subflow_type_agg and normalization_key, aggregating the normalized values\n",
    "energy_au_mj_norm_agg = (\n",
    "    energy_au_mj_norm\n",
    "    .groupby(\n",
    "        [\n",
    "            'main_id', 'facility_group_id', 'company_id',\n",
    "            'year', 'flow_type', 'subflow_type_agg'],\n",
    "        dropna=False, as_index=False\n",
    "    )\n",
    "    .agg(value_normalized_sum=('value_normalized', 'sum'))\n",
    ")"
   ],
   "id": "cbe5ed80cfff33db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Biosphere",
   "id": "48f597b710bba4e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "biosphere_au = env_table[\n",
    "    env_table['main_id'].isin(au_ids) | env_table['facility_group_id'].isin(au_ids)\n",
    "].copy()"
   ],
   "id": "7d9cf594d69178a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "biosphere_au",
   "id": "b0754128fd3cb03b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "biosphere_au.to_csv(r'biosphere_au.csv', index=False)",
   "id": "e206c96720f1d0a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parametrization",
   "id": "e5304555339e8c2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ta_au = tech_attributes_table[\n",
    "    tech_attributes_table['main_id'].isin(au_ids) | tech_attributes_table['facility_group_id'].isin(au_ids)\n",
    "].copy()"
   ],
   "id": "403d2c8a7c30146",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ta_au",
   "id": "6e0fe6cda7cc001f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ta_au",
   "id": "2a86180a070e7b0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ta_au['value_%'] = ta_au.apply(convert_to_percent, axis=1)\n",
    "ta_au.drop(columns=['unit', 'value'], inplace=True)"
   ],
   "id": "c265ee722ee07d95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "archetypes_au = archetypes_table[\n",
    "    (archetypes_table['main_id'].isin(au_ids) | archetypes_table['facility_group_id'].isin(au_ids))\n",
    "].copy()"
   ],
   "id": "c933cbc3bc648252",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parametrization_au = merge_without_suffixes(ta_au, archetypes_au, keys=(\"main_id\", \"facility_group_id\"), how=\"left\")",
   "id": "c9f3985955a52259",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parametrization_au",
   "id": "f8806e6e1ba26e47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "energy_au_mj_norm.to_csv(\"results/energy_au_mj_norm.csv\", index=False)\n",
    "parametrization_au.to_csv(\"results/parametrization_au.csv\", index=False)"
   ],
   "id": "21867fdf761c40c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "energy_df_mj_norm_agg",
   "id": "ad172b466c890445",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "parametrization_au",
   "id": "506e6a3635c8434c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot data",
   "id": "407d3bec8b0f652a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "energy_grade = merge_without_suffixes(energy_df_mj_norm_agg, parametrization_au, keys=(\"main_id\", \"facility_group_id\"), how=\"left\")",
   "id": "27c25b232bed6197",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "energy_grade",
   "id": "23d858101f9ea060",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stripplot_energy_grade_df = stripplot_with_barplot(\n",
    "    energy_grade,\n",
    "    \"value_%\",\n",
    "    \"value_normalized_sum\",\n",
    "    hue=\"mining_method\",\n",
    "    log_scale=False,\n",
    "    outfile=\"results/stripplot_energy_grade.pdf\",\n",
    "    dpi=600\n",
    ")"
   ],
   "id": "142034d3631912eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f188e676dcae3cc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "61141a9852630ddb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
